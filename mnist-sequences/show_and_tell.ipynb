{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from time import time\n",
    "import math\n",
    "import cPickle as pickle\n",
    "from utils import save_array, load_array, one_hot_encode, time_since"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the convolutional feature vectors\n",
    "train_features = pickle.load(open(\"data/train_features.pkl\", \"rb\"))\n",
    "test_features = pickle.load(open(\"data/test_features.pkl\", \"rb\"))\n",
    "train_labels = load_array(\"data/train_labels.bc\")\n",
    "test_labels = load_array(\"data/test_labels.bc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 32, 7, 35])\n",
      "(500, 5)\n",
      "torch.Size([200, 32, 7, 35])\n",
      "(200, 5)\n"
     ]
    }
   ],
   "source": [
    "# inspect the dimensions of the convolutional features\n",
    "print(train_features.size())\n",
    "print(train_labels.shape)\n",
    "print(test_features.size())\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7840]) torch.Size([5])\n",
      "torch.Size([7840]) torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# preprocess the convolutional features: Flatten the inputs and convert labels to LongTensors\n",
    "train_data = []\n",
    "for i in range(train_features.size()[0]):\n",
    "    train_data.append((train_features[i].view(-1), torch.LongTensor(train_labels[i])))\n",
    "\n",
    "test_data = []\n",
    "for i in range(test_features.size()[0]):\n",
    "    test_data.append((test_features[i].view(-1), torch.LongTensor(test_labels[i])))\n",
    "    \n",
    "# inspect the dimensions of a single training and test example\n",
    "print(train_data[0][0].size(), train_data[0][1].shape)\n",
    "print(test_data[0][0].size(), test_data[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "vocab_size = 10\n",
    "embedding_dim = 7840\n",
    "hidden_dim = 1024\n",
    "dropout_prob = 0.1\n",
    "target_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An LSTM to classify digit sequences by making use of convolutional input features\n",
    "class MNIST_Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_prob):\n",
    "        super(MNIST_Seq, self).__init__()\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.decoder = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "\n",
    "    def init_hidden(self):\n",
    "        self.hidden = (Variable(torch.zeros(1, 1, self.hidden_dim)).cuda(),\n",
    "                       Variable(torch.zeros(1, 1, self.hidden_dim)).cuda())\n",
    "\n",
    "    def forward(self, context):\n",
    "        lstm_out, self.hidden = self.lstm(context, self.hidden)\n",
    "        lstm_dropped = self.dropout(lstm_out)\n",
    "        decoded = self.decoder(lstm_dropped)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    model.init_hidden()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # feed the convolutional features as input at each time step\n",
    "    input_var = Variable(torch.stack([inp] * target_size)).cuda()\n",
    "\n",
    "    output = model(input_var.view(target_size, 1, -1))\n",
    "    loss = criterion(output.view(target_size, vocab_size), Variable(target).cuda())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_size\n",
    "\n",
    "def evaluate(data):\n",
    "    model.eval()\n",
    "    correct_digits = 0\n",
    "    total_digits = 0\n",
    "    for this_input, this_target in data:\n",
    "        model.init_hidden()\n",
    "        input_var = Variable(torch.stack([this_input] * target_size)).cuda()\n",
    "        output = model(input_var.view(target_size,1,-1))\n",
    "        # get predictions\n",
    "        top_n, top_i = torch.topk(output, 1, dim=2)\n",
    "        pred = top_i.cpu().data.view(5)\n",
    "        correct_digits += (pred == this_target).sum()\n",
    "        total_digits += target_size\n",
    "#         print(\"pred: \", pred.numpy())\n",
    "#         print(\"targ: \", this_target.numpy())\n",
    "#         print()\n",
    "    return float(correct_digits) / total_digits\n",
    "\n",
    "def train_epoch():\n",
    "    model.train(True)\n",
    "    for this_input, this_target in train_data:\n",
    "        loss = train(this_input, this_target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = MNIST_Seq(vocab_size, embedding_dim, hidden_dim, dropout_prob).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some training settings\n",
    "n_epochs=25\n",
    "print_every = 1\n",
    "plot_every = 10\n",
    "all_losses = []\n",
    "loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use this cell to update the learning rate between epochs\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 21s (1 4%) 0.4488 (train_acc: 0.336, test_acc: 0.316)]\n",
      "[0m 43s (2 8%) 0.3666 (train_acc: 0.383, test_acc: 0.345)]\n",
      "[1m 5s (3 12%) 0.3279 (train_acc: 0.439, test_acc: 0.369)]\n",
      "[1m 27s (4 16%) 0.2935 (train_acc: 0.514, test_acc: 0.429)]\n",
      "[1m 49s (5 20%) 0.2517 (train_acc: 0.548, test_acc: 0.460)]\n",
      "[2m 11s (6 24%) 0.2208 (train_acc: 0.607, test_acc: 0.497)]\n",
      "[2m 33s (7 28%) 0.2212 (train_acc: 0.655, test_acc: 0.513)]\n",
      "[2m 54s (8 32%) 0.1822 (train_acc: 0.704, test_acc: 0.539)]\n",
      "[3m 16s (9 36%) 0.2483 (train_acc: 0.726, test_acc: 0.564)]\n",
      "[3m 38s (10 40%) 0.1750 (train_acc: 0.752, test_acc: 0.565)]\n",
      "[4m 0s (11 44%) 0.1193 (train_acc: 0.772, test_acc: 0.584)]\n",
      "[4m 22s (12 48%) 0.1198 (train_acc: 0.779, test_acc: 0.607)]\n",
      "[4m 44s (13 52%) 0.1168 (train_acc: 0.794, test_acc: 0.619)]\n",
      "[5m 6s (14 56%) 0.1049 (train_acc: 0.829, test_acc: 0.626)]\n",
      "[5m 28s (15 60%) 0.1037 (train_acc: 0.854, test_acc: 0.642)]\n",
      "[5m 49s (16 64%) 0.0744 (train_acc: 0.872, test_acc: 0.684)]\n",
      "[6m 11s (17 68%) 0.0709 (train_acc: 0.903, test_acc: 0.691)]\n",
      "[6m 33s (18 72%) 0.0663 (train_acc: 0.891, test_acc: 0.684)]\n",
      "[6m 55s (19 76%) 0.0651 (train_acc: 0.830, test_acc: 0.655)]\n",
      "[7m 17s (20 80%) 0.0492 (train_acc: 0.920, test_acc: 0.701)]\n",
      "[7m 39s (21 84%) 0.0447 (train_acc: 0.944, test_acc: 0.724)]\n",
      "[8m 1s (22 88%) 0.0605 (train_acc: 0.924, test_acc: 0.708)]\n",
      "[8m 23s (23 92%) 0.0265 (train_acc: 0.967, test_acc: 0.749)]\n",
      "[8m 44s (24 96%) 0.0271 (train_acc: 0.980, test_acc: 0.755)]\n",
      "[9m 6s (25 100%) 0.0198 (train_acc: 0.979, test_acc: 0.753)]\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate\n",
    "start = time()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train_epoch()\n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        \n",
    "        train_acc = evaluate(train_data)\n",
    "        test_acc = evaluate(test_data)\n",
    "        print('[%s (%d %d%%) %.4f (train_acc: %.3f, test_acc: %.3f)]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, train_acc, test_acc))\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
