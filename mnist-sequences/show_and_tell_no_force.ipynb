{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from time import time\n",
    "import math\n",
    "import bcolz\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_array(fname, arr): \n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "    \n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the convolutional feature vectors\n",
    "train_features = pickle.load(open(\"data/train_features.pkl\", \"rb\"))\n",
    "test_features = pickle.load(open(\"data/test_features.pkl\", \"rb\"))\n",
    "train_labels = load_array(\"data/train_labels.bc\")\n",
    "test_labels = load_array(\"data/test_labels.bc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 32, 7, 35])\n",
      "(500, 5)\n",
      "torch.Size([200, 32, 7, 35])\n",
      "(200, 5)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.size())\n",
    "print(train_labels.shape)\n",
    "print(test_features.size())\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7840]) torch.Size([5])\n",
      "torch.Size([7840]) torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "for i in range(train_features.size()[0]):\n",
    "    train_data.append((train_features[i].view(-1), torch.LongTensor(train_labels[i])))\n",
    "print(train_data[0][0].size(), train_data[0][1].shape)\n",
    "\n",
    "test_data = []\n",
    "for i in range(test_features.size()[0]):\n",
    "    test_data.append((test_features[i].view(-1), torch.LongTensor(test_labels[i])))\n",
    "print(test_data[0][0].size(), test_data[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 10\n",
    "embedding_dim = 7840\n",
    "hidden_dim = 1024\n",
    "dropout_prob = 0.1\n",
    "target_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    s = time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train an LSTM to recognize digit sequences by using the convolutional features\n",
    "class MNIST_Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_prob):\n",
    "        super(MNIST_Seq, self).__init__()\n",
    "\n",
    "        self.hidden_dim=hidden_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.decoder = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "\n",
    "    def init_hidden(self):\n",
    "        self.hidden = (Variable(torch.zeros(1, 1, self.hidden_dim)).cuda(),\n",
    "                        Variable(torch.zeros(1, 1, self.hidden_dim)).cuda())\n",
    "\n",
    "    def forward(self, context):\n",
    "        lstm_out, self.hidden = self.lstm(context, self.hidden)\n",
    "        lstm_dropped = self.dropout(lstm_out)\n",
    "        decoded = self.decoder(lstm_dropped)\n",
    "        #logprobs = F.log_softmax(decoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    model.eval()\n",
    "    correct_digits = 0\n",
    "    total_digits = 0\n",
    "    for this_input, this_target in data:\n",
    "        model.init_hidden()\n",
    "        input_var = Variable(torch.stack([this_input] * target_size)).cuda()\n",
    "        output = model(input_var.view(target_size,1,-1))\n",
    "        top_n, top_i = torch.topk(output, 1, dim=2)\n",
    "        pred = top_i.cpu().data.view(5)\n",
    "        correct_digits += (pred == this_target).sum()\n",
    "        total_digits += 5\n",
    "#         print(\"pred: \", pred.numpy())\n",
    "#         print(\"targ: \", this_target.numpy())\n",
    "#         print()\n",
    "    return float(correct_digits) / total_digits\n",
    "\n",
    "def train(inp, target):\n",
    "    model.init_hidden()\n",
    "    model.zero_grad()\n",
    "\n",
    "    input_var = Variable(torch.stack([inp] * target_size)).cuda()\n",
    "\n",
    "    output = model(input_var.view(target_size, 1, -1))\n",
    "    loss = criterion(output.view(target_size, vocab_size), Variable(target).cuda())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_size\n",
    "\n",
    "def train_epoch():\n",
    "    model.train(True)\n",
    "    for this_input, this_target in train_data:\n",
    "        loss = train(this_input, this_target)\n",
    "        #print loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MNIST_Seq(vocab_size, embedding_dim, hidden_dim, dropout_prob).cuda()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs=25\n",
    "print_every = 1\n",
    "plot_every = 10\n",
    "\n",
    "all_losses = []\n",
    "loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 21s (1 4%) 0.4488 (train_acc: 0.336, test_acc: 0.316)]\n",
      "[0m 43s (2 8%) 0.3666 (train_acc: 0.383, test_acc: 0.345)]\n",
      "[1m 5s (3 12%) 0.3279 (train_acc: 0.439, test_acc: 0.369)]\n",
      "[1m 27s (4 16%) 0.2935 (train_acc: 0.514, test_acc: 0.429)]\n",
      "[1m 49s (5 20%) 0.2517 (train_acc: 0.548, test_acc: 0.460)]\n",
      "[2m 11s (6 24%) 0.2208 (train_acc: 0.607, test_acc: 0.497)]\n",
      "[2m 33s (7 28%) 0.2212 (train_acc: 0.655, test_acc: 0.513)]\n",
      "[2m 54s (8 32%) 0.1822 (train_acc: 0.704, test_acc: 0.539)]\n",
      "[3m 16s (9 36%) 0.2483 (train_acc: 0.726, test_acc: 0.564)]\n",
      "[3m 38s (10 40%) 0.1750 (train_acc: 0.752, test_acc: 0.565)]\n",
      "[4m 0s (11 44%) 0.1193 (train_acc: 0.772, test_acc: 0.584)]\n",
      "[4m 22s (12 48%) 0.1198 (train_acc: 0.779, test_acc: 0.607)]\n",
      "[4m 44s (13 52%) 0.1168 (train_acc: 0.794, test_acc: 0.619)]\n",
      "[5m 6s (14 56%) 0.1049 (train_acc: 0.829, test_acc: 0.626)]\n",
      "[5m 28s (15 60%) 0.1037 (train_acc: 0.854, test_acc: 0.642)]\n",
      "[5m 49s (16 64%) 0.0744 (train_acc: 0.872, test_acc: 0.684)]\n",
      "[6m 11s (17 68%) 0.0709 (train_acc: 0.903, test_acc: 0.691)]\n",
      "[6m 33s (18 72%) 0.0663 (train_acc: 0.891, test_acc: 0.684)]\n",
      "[6m 55s (19 76%) 0.0651 (train_acc: 0.830, test_acc: 0.655)]\n",
      "[7m 17s (20 80%) 0.0492 (train_acc: 0.920, test_acc: 0.701)]\n",
      "[7m 39s (21 84%) 0.0447 (train_acc: 0.944, test_acc: 0.724)]\n",
      "[8m 1s (22 88%) 0.0605 (train_acc: 0.924, test_acc: 0.708)]\n",
      "[8m 23s (23 92%) 0.0265 (train_acc: 0.967, test_acc: 0.749)]\n",
      "[8m 44s (24 96%) 0.0271 (train_acc: 0.980, test_acc: 0.755)]\n",
      "[9m 6s (25 100%) 0.0198 (train_acc: 0.979, test_acc: 0.753)]\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train_epoch()\n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        \n",
    "        train_acc = evaluate(train_data)\n",
    "        test_acc = evaluate(test_data)\n",
    "        print('[%s (%d %d%%) %.4f (train_acc: %.3f, test_acc: %.3f)]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, train_acc, test_acc))\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
