{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from time import time\n",
    "import math\n",
    "import bcolz\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(value, value_range=11):\n",
    "    one_hot = torch.zeros(value_range)\n",
    "    one_hot[value] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_array(fname, arr): \n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "    \n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the convolutional feature vectors\n",
    "train_features = pickle.load(open(\"data/train_features.pkl\", \"rb\"))\n",
    "test_features = pickle.load(open(\"data/test_features.pkl\", \"rb\"))\n",
    "train_labels = load_array(\"data/train_labels.bc\")\n",
    "test_labels = load_array(\"data/test_labels.bc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 32, 7, 35])\n",
      "(500, 5)\n",
      "torch.Size([200, 32, 7, 35])\n",
      "(200, 5)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.size())\n",
    "print(train_labels.shape)\n",
    "print(test_features.size())\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7840]) torch.Size([5])\n",
      "torch.Size([7840]) torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "for i in range(train_features.size()[0]):\n",
    "    train_data.append((train_features[i].view(-1), torch.LongTensor(train_labels[i])))\n",
    "print(train_data[0][0].size(), train_data[0][1].shape)\n",
    "\n",
    "test_data = []\n",
    "for i in range(test_features.size()[0]):\n",
    "    test_data.append((test_features[i].view(-1), torch.LongTensor(test_labels[i])))\n",
    "print(test_data[0][0].size(), test_data[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 10\n",
    "embedding_dim = 7840 + 11\n",
    "hidden_dim = 1024\n",
    "dropout_prob = 0\n",
    "target_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    s = time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train an LSTM to recognize digit sequences by using the convolutional features\n",
    "class MNIST_Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_prob):\n",
    "        super(MNIST_Seq, self).__init__()\n",
    "\n",
    "        self.hidden_dim=hidden_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.decoder = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "\n",
    "    def init_hidden(self):\n",
    "        self.hidden = (Variable(torch.zeros(1, 1, self.hidden_dim)).cuda(),\n",
    "                        Variable(torch.zeros(1, 1, self.hidden_dim)).cuda())\n",
    "\n",
    "    def forward(self, context):\n",
    "        lstm_out, self.hidden = self.lstm(context, self.hidden)\n",
    "        lstm_dropped = self.dropout(lstm_out)\n",
    "        decoded = self.decoder(lstm_dropped)\n",
    "        #logprobs = F.log_softmax(decoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    model.eval()\n",
    "    correct_digits = 0\n",
    "    total_digits = 0\n",
    "    for this_input, this_target in data:\n",
    "        model.init_hidden()\n",
    "        # input_char is initially an SOS char\n",
    "        input_char = one_hot_encode(10, 11)\n",
    "        predictions = torch.LongTensor(target_size)\n",
    "        for i in range(target_size):\n",
    "            inpt = Variable(torch.cat([this_input, input_char])).cuda()\n",
    "            output = model(inpt.view(1,1,-1))\n",
    "            # get pred\n",
    "            top_n, top_i = torch.topk(output, 1, dim=2)\n",
    "            pred = top_i.cpu().data.view(1)[0]\n",
    "            predictions[i] = pred\n",
    "            input_char = one_hot_encode(pred, 11)             \n",
    "#         print(predictions)\n",
    "#         print(this_target)\n",
    "        correct_digits += (predictions == this_target).sum()\n",
    "        total_digits += 5\n",
    "#         print(\"pred: \", predictions.numpy())\n",
    "#         print(\"targ: \", this_target.numpy())\n",
    "#         print()\n",
    "    return float(correct_digits) / total_digits\n",
    "\n",
    "def train(inp, target):\n",
    "    model.init_hidden()\n",
    "    model.zero_grad()\n",
    "\n",
    "    input_im = torch.stack([inp] * target_size)\n",
    "    \n",
    "    ls = [10] + list(target[:-1])\n",
    "    ls = [one_hot_encode(value, 11) for value in ls]\n",
    "    input_char = torch.stack(ls)\n",
    "    \n",
    "    inpt = Variable(torch.cat([input_im, input_char], 1)).cuda()\n",
    "    \n",
    "    output = model(inpt.view(target_size, 1, -1))\n",
    "    loss = criterion(output.view(target_size, vocab_size), Variable(target).cuda())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_size\n",
    "\n",
    "def train_epoch():\n",
    "    model.train(True)\n",
    "    for this_input, this_target in train_data:\n",
    "        loss = train(this_input, this_target)\n",
    "        #print loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MNIST_Seq(vocab_size, embedding_dim, hidden_dim, dropout_prob).cuda()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs=5\n",
    "print_every = 1\n",
    "plot_every = 10\n",
    "\n",
    "all_losses = []\n",
    "loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 24s (1 20%) 0.4512 (train_acc: 0.343, test_acc: 0.313)]\n",
      "[0m 49s (2 40%) 0.3603 (train_acc: 0.398, test_acc: 0.357)]\n",
      "[1m 13s (3 60%) 0.3186 (train_acc: 0.463, test_acc: 0.390)]\n",
      "[1m 38s (4 80%) 0.2749 (train_acc: 0.516, test_acc: 0.427)]\n",
      "[2m 3s (5 100%) 0.2475 (train_acc: 0.570, test_acc: 0.465)]\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "    loss = train_epoch()\n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        \n",
    "        train_acc = evaluate(train_data)\n",
    "        test_acc = evaluate(test_data)\n",
    "        print('[%s (%d %d%%) %.4f (train_acc: %.3f, test_acc: %.3f)]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, train_acc, test_acc))\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
